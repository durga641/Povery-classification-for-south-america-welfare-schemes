# -*- coding: utf-8 -*-
"""
Created on Mon Sep  3 15:16:43 2018

@author: vuppuluri
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Sep  3 11:13:06 2018

@author: vuppuluri
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
from collections import Counter
from sklearn import model_selection
import seaborn as sns
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory


##print(os.listdir("../input"))
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing
# Any results you write to the current directory are saved as output.
os.chdir("D:\Data Science\costatica_poverty_class")

##os.chdir("../input")    ## if you using kaggle
train=pd.read_csv('train.csv')
test=pd.read_csv("test.csv")

#### impute missing values
train=train.replace({'dependency': 'no','edjefa':'no','edjefe':'no','v18q1': np.nan}, 0)
train=train.replace({'dependency': 'yes','edjefa':'yes','edjefe':'yes'}, np.nan)
#train=train.replace({'rez_esc': np.nan}, 100)


mean_imputer = preprocessing.Imputer() #By defalut parameter is mean and let it use default one.
mean_imputer.fit(train[['v2a1','meaneduc','SQBmeaned','dependency','edjefa','edjefe','rez_esc']]) 
train[['v2a1','meaneduc','SQBmeaned','dependency','edjefa','edjefe','rez_esc']] = mean_imputer.transform(train[['v2a1','meaneduc','SQBmeaned','dependency','edjefa','edjefe','rez_esc']])



print(np.nan)

#### droping columns with object data type
x_train=train.drop(columns=['idhogar','Id'])
x_train=x_train.drop_duplicates()

########## make test dat accommadated with similar changes
test=test.replace({'dependency': 'no','edjefa':'no','edjefe':'no','v18q1': np.nan}, 0)
test=test.replace({'dependency': 'yes','edjefa':'yes','edjefe':'yes'}, np.nan)
mean_imputer = preprocessing.Imputer() #By defalut parameter is mean and let it use default one.
mean_imputer.fit(test[['v2a1','meaneduc','SQBmeaned','dependency','edjefa','edjefe','rez_esc']]) 
test[['v2a1','meaneduc','SQBmeaned','dependency','edjefa','edjefe','rez_esc']] = mean_imputer.transform(test[['v2a1','meaneduc','SQBmeaned','dependency','edjefa','edjefe','rez_esc']])


x_test=test.drop(columns=['idhogar','Id'])


############################# DETECT OUTLIERS ############################


def detect_outliers(df,n,features):
    """
    Takes a dataframe df of features and returns a list of the indices
    corresponding to the observations containing more than n outliers according
    to the Tukey method.
    """
    outlier_indices = []
    
    # iterate over features(columns)
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col],75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        
        # outlier step
        outlier_step = 1.5 * IQR
        
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index
        
        # append the found outlier indices for col to the list of outlier indices 
        outlier_indices.extend(outlier_list_col)
        
    # select observations containing more than 2 outliers
    outlier_indices = Counter(outlier_indices)        
    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )
    
    return multiple_outliers   

# detect outliers from all fields
Outliers_to_drop = detect_outliers(x_train,12,list(x_train))

x_train = x_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)
y_train=x_train['Target']
x_train=x_train.drop(columns=['Target'])

x_train.to_csv("model_input.csv",index=False)




kfold=model_selection.StratifiedKFold(n_splits=14,random_state=105641)


############ RANDOM FOREST###################3
rdt=RandomForestClassifier()
dt_grid = {'max_depth':list(range(3,8)), 'min_samples_split':[2,3,6,7,8], 'criterion':['gini','entropy'],'n_estimators':list(range(50,201,50))}
grid_tree_estimator = model_selection.GridSearchCV(rdt,dt_grid,cv=kfold,scoring='f1_macro')
grid_tree_estimator.fit(x_train, y_train)


x_test['Target']=grid_tree_estimator.predict(x_test)
x_test['Id']=test[['Id']]
grid_tree_estimator.best_score_

x_test.to_csv('submission.csv',columns=['Id','Target'],index=False)

##,'n_estimators':list(range(50,201,50))

## earlier :65.9







